{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrnakaha2\u001b[0m (\u001b[33mbinghamton-ml\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import pprint\n",
    "from glob import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import Transformer_Model as Model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'Sweep_wandb.ipynb'\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'minimize', 'name': 'loss'},\n",
      " 'parameters': {'batch_size': {'distribution': 'q_log_uniform_values',\n",
      "                               'max': 64,\n",
      "                               'min': 8,\n",
      "                               'q': 8},\n",
      "                'epochs': {'value': 5},\n",
      "                'ff_dim': {'values': [300, 350, 400, 450]},\n",
      "                'hidden_dim': {'values': [150, 200, 250]},\n",
      "                'learning_rate': {'distribution': 'uniform',\n",
      "                                  'max': 0.001,\n",
      "                                  'min': 1e-08}}}\n"
     ]
    }
   ],
   "source": [
    "num_trails = 30\n",
    "sweep_config = {'method': 'random'}\n",
    "metric = {'name': 'loss','goal': 'minimize'}\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'ff_dim': {'values':[300, 350, 400, 450, 500]},\n",
    "    'hidden_dim':{'values':[150, 200 , 250, 300]}\n",
    "  }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "parameters_dict.update({'epochs': {'value': 10}})\n",
    "\n",
    "parameters_dict.update({\n",
    "  'learning_rate': {'distribution': 'uniform','min': 0.00000001,'max': 0.001},\n",
    "  'beta1': {'distribution': 'uniform','min': 0.6,'max': 0.97},\n",
    "  'beta2': {'distribution': 'uniform','min': 0.7,'max': 1},\n",
    "  'batch_size': {'distribution': 'q_log_uniform_values','q': 8,'min': 16,'max': 64}\n",
    "  })\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(wavs, id_to_text, maxlen=50):\n",
    "  \"\"\" returns mapping of audio paths and transcription texts \"\"\"\n",
    "  data = []\n",
    "  for w in wavs:\n",
    "    id = w.split(\"/\")[-1].split(\".\")[0]\n",
    "    if len(id_to_text[id]) < maxlen:\n",
    "      data.append({\"audio\": w, \"text\": id_to_text[id]})\n",
    "  return data\n",
    "\n",
    "class VectorizeChar:\n",
    "  def __init__(self, max_len=50):\n",
    "    self.vocab = (\n",
    "      [\"-\", \"#\", \"<\", \">\"]\n",
    "      + [chr(i + 96) for i in range(1, 27)]\n",
    "      + [\" \", \".\", \",\", \"?\"]\n",
    "    )\n",
    "    self.max_len = max_len\n",
    "    self.char_to_idx = {}\n",
    "    for i, ch in enumerate(self.vocab):\n",
    "      self.char_to_idx[ch] = i\n",
    "\n",
    "  def __call__(self, text):\n",
    "    text = text.lower()\n",
    "    text = text[: self.max_len - 2]\n",
    "    text = \"<\" + text + \">\"\n",
    "    pad_len = self.max_len - len(text)\n",
    "    return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
    "\n",
    "  def get_vocabulary(self):\n",
    "    return self.vocab\n",
    "  \n",
    "\n",
    "def path_2_audio(path):\n",
    "  waveform, _ = torchaudio.load(path)\n",
    "  audio = torch.squeeze(waveform, dim=0)\n",
    "  stfts = torch.stft(audio, n_fft=256, hop_length=80, win_length=200, return_complex=True)\n",
    "  x = torch.pow(torch.abs(stfts), 0.5)\n",
    "  means = torch.mean(x, 1, keepdims=True)\n",
    "  stddevs = torch.std(x, 1, keepdims=True)\n",
    "  x = (x - means) / stddevs\n",
    "  pad_len = 2754\n",
    "  paddings = (0, pad_len, 0, 0)\n",
    "  x = F.pad(x, paddings, 'constant',0)[:, :pad_len]\n",
    "  return x\n",
    "\n",
    "def create_text_ds(data):\n",
    "  vectorizer = VectorizeChar(max_len=200)\n",
    "  texts = [_[\"text\"] for _ in data]\n",
    "  text_ds = [vectorizer(t) for t in texts]\n",
    "  return text_ds\n",
    "\n",
    "\n",
    "class AudioTextDataset(Dataset):\n",
    "  def __init__(self, raw_data):\n",
    "    self.audio = [_[\"audio\"] for _ in raw_data]\n",
    "    self.text = create_text_ds(raw_data)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "  def __getitem__(self, i):\n",
    "    audio = path_2_audio(self.audio[i])\n",
    "    text = torch.tensor(self.text[i])\n",
    "    return audio, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "  dir = \"/home/rnakaha2/documents/speech/LJSpeech-1.1\"\n",
    "  wavs = glob(\"{}/**/*.wav\".format(dir), recursive=True)\n",
    "  id_to_text = {}\n",
    "  with open(os.path.join(dir, \"metadata.csv\"), encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "      id = line.strip().split(\"|\")[0]\n",
    "      text = line.strip().split(\"|\")[2]\n",
    "      id_to_text[id] = text\n",
    "  raw_data = get_data(wavs, id_to_text, maxlen=200)\n",
    "  return raw_data\n",
    "\n",
    "raw_data = load_data()\n",
    "\n",
    "def build_dataset(raw_data, batch_size):\n",
    "  dataset = AudioTextDataset(raw_data)\n",
    "  trn_dl = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "  return trn_dl\n",
    "\n",
    "def build_network(batch_size, hidden_dim, ff_dim):\n",
    "  model = Model.Transformer(batch_size=batch_size, num_hid=hidden_dim, num_head=2, num_feed_forward=ff_dim, \n",
    "           num_layers_enc=4, num_layers_dec=1).to(device)\n",
    "  return model\n",
    "\n",
    "def generate(model, source, target_start_token_idx):\n",
    "  source = source.to(device)\n",
    "  bs = source.shape[0]\n",
    "  enc = model.encoder(source)\n",
    "  dec_input = torch.ones((bs, 1), dtype=torch.int32) * target_start_token_idx\n",
    "  dec_input = dec_input.to(device)\n",
    "  for i in range(model.target_maxlen - 1):\n",
    "    dec_out = model.decoder(enc, dec_input, 1)\n",
    "    logits = model.classifier(dec_out)\n",
    "    #logits = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "    logits = torch.argmax(logits, dim=-1)\n",
    "    last_logit = torch.unsqueeze(logits[:, -1], axis=1)\n",
    "    dec_input = torch.cat((dec_input, last_logit), axis=-1)\n",
    "  return dec_input\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Apply Xavier/Glorot initialization to linear layers\n",
    "            init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                init.constant_(module.bias, 0.0)\n",
    "        elif isinstance(module, nn.Conv2d) or isinstance(module, nn.Conv1d):\n",
    "            # Apply Xavier/Glorot initialization to convolutional layers\n",
    "            init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                init.constant_(module.bias, 0.0)\n",
    "\n",
    "def build_optimizer(network, learning_rate, beta1, beta2):\n",
    "  optimizer = optim.AdamW(network.parameters(), lr=learning_rate, betas=(beta1, beta2))\n",
    "  return optimizer\n",
    "\n",
    "def network_loss(pred, target):\n",
    "  lossfn = nn.CrossEntropyLoss(ignore_index=0)  # Use CrossEntropyLoss\n",
    "  pred = pred.transpose(1, 2)  # Transpose predictions to match the shape expected by CrossEntropyLoss\n",
    "  loss = lossfn(pred, target)\n",
    "  return loss\n",
    "\n",
    "def train_epoch(network, loader, optimizer):\n",
    "  cumu_loss = 0\n",
    "  audio, text = None, None\n",
    "  for _, (audio, text) in enumerate(loader):\n",
    "    audio, text = audio.to(device), text.to(device)\n",
    "    dec_input = text[:, :-1]\n",
    "    dec_target = text[:, 1:]\n",
    "    optimizer.zero_grad()\n",
    "    network.train()\n",
    "    preds = network(audio, dec_input)\n",
    "    preds = F.log_softmax(preds, dim=-1)\n",
    "\n",
    "    loss = network_loss(preds, dec_target)\n",
    "    cumu_loss += loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    wandb.log({\"batch loss\": loss.item()})\n",
    "  vectorizer = VectorizeChar(max_len=200)\n",
    "  idx_to_token = vectorizer.get_vocabulary()\n",
    "  preds = generate(network, audio[0:2], 2)\n",
    "  preds = preds.cpu().detach().numpy()\n",
    "  target_text = \"\".join([idx_to_token[_] for _ in text[0, :]])\n",
    "  prediction = \"\"\n",
    "  for idx in preds[0, :]:\n",
    "      prediction += idx_to_token[idx]\n",
    "      if idx == 3:\n",
    "        break\n",
    "  target = target_text.replace('-','')\n",
    "  return cumu_loss / len(loader), target, prediction\n",
    "\n",
    "\n",
    "def train(config=None):\n",
    "  with wandb.init(config=config):\n",
    "    table = wandb.Table(columns=[\"Epoch\", \"Loss\",\"Prediction\", \"Target\"])\n",
    "    config = wandb.config\n",
    "    loader = build_dataset(raw_data, config.batch_size)\n",
    "    network = build_network(config.batch_size, config.hidden_dim, config.ff_dim)\n",
    "    initialize_weights(network)\n",
    "    optimizer = build_optimizer(network, config.learning_rate, config.beta1, config.beta2)\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "      avg_loss, target, prediction = train_epoch(network, loader, optimizer)\n",
    "      wandb.log({\"loss\": avg_loss, \"epoch\": epoch})\n",
    "      table.add_data(epoch, avg_loss, prediction, target)\n",
    "    wandb.log({\"result\": table})\n",
    "    del network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: k76ek8ms\n",
      "Sweep URL: https://wandb.ai/binghamton-ml/ASR_Model/sweeps/k76ek8ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ta7fpvin with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tff_dim: 400\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 200\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00015905943088076517\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rnakaha2/documents/speech/Training/wandb/run-20230626_175547-ta7fpvin</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/binghamton-ml/ASR_Model/runs/ta7fpvin' target=\"_blank\">ancient-sweep-1</a></strong> to <a href='https://wandb.ai/binghamton-ml/ASR_Model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/binghamton-ml/ASR_Model/sweeps/k76ek8ms' target=\"_blank\">https://wandb.ai/binghamton-ml/ASR_Model/sweeps/k76ek8ms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/binghamton-ml/ASR_Model' target=\"_blank\">https://wandb.ai/binghamton-ml/ASR_Model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/binghamton-ml/ASR_Model/sweeps/k76ek8ms' target=\"_blank\">https://wandb.ai/binghamton-ml/ASR_Model/sweeps/k76ek8ms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/binghamton-ml/ASR_Model/runs/ta7fpvin' target=\"_blank\">https://wandb.ai/binghamton-ml/ASR_Model/runs/ta7fpvin</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b153dc847f5545ba9aa58a43cb952c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.007 MB of 0.008 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.814677…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>█▆▆▅▅▅▅▅▅▄▄▄▃▄▃▄▄▃▃▃▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>loss</td><td>█▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch loss</td><td>1.85482</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>1.87676</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ancient-sweep-1</strong> at: <a href='https://wandb.ai/binghamton-ml/ASR_Model/runs/ta7fpvin' target=\"_blank\">https://wandb.ai/binghamton-ml/ASR_Model/runs/ta7fpvin</a><br/>Synced 4 W&B file(s), 1 media file(s), 1 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230626_175547-ta7fpvin/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tuegs79w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tff_dim: 350\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 250\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00021884818009060663\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/rnakaha2/documents/speech/Training/wandb/run-20230626_180557-tuegs79w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/binghamton-ml/ASR_Model/runs/tuegs79w' target=\"_blank\">stellar-sweep-2</a></strong> to <a href='https://wandb.ai/binghamton-ml/ASR_Model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/binghamton-ml/ASR_Model/sweeps/k76ek8ms' target=\"_blank\">https://wandb.ai/binghamton-ml/ASR_Model/sweeps/k76ek8ms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/binghamton-ml/ASR_Model' target=\"_blank\">https://wandb.ai/binghamton-ml/ASR_Model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/binghamton-ml/ASR_Model/sweeps/k76ek8ms' target=\"_blank\">https://wandb.ai/binghamton-ml/ASR_Model/sweeps/k76ek8ms</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/binghamton-ml/ASR_Model/runs/tuegs79w' target=\"_blank\">https://wandb.ai/binghamton-ml/ASR_Model/runs/tuegs79w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"ASR_Model_Sweep\")\n",
    "wandb.agent(sweep_id, train, count=num_trails)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
